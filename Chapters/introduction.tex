\doublespacing
\section{Hypertension}

\subsection{A Rise in Blood Pressure}

In 1628, the English physician William Harvey published a pioneering description of the functionality of the heart and blood. In his seminal \textit{Exercitatio Anatomica de Motu Cordis et Sanguinis in Animalibus} (Anatomical Account of the Motion of the Heart and Blood in Animals), Harvey was the first to observe the properties of the blood, and its integrated systemic circulation around the body by the heart. In doing so, he gave rise to the modern history of cardiovascular research and mankind's understanding of its importance (Figure \ref{fig:harvey}). Prior to his work, blood was thought to be separated between venous and arterial systems that were quite separate, and the heart simply considered an internal regulator of heat. Over 100 years later, Stephen Hales took Harvey's work and further propelled the field with his first quantitative measurements of blood pressure \cite{hales}. Using finely produced glass columns inserted into the arteries of animal subjects, Hales could note the height to which the column of blood rose; giving rise to a comparative metric of blood pressure. \\

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{Introduction/harvey.jpg}
\caption[An illustration from an experiment in Harvey's \textit{De Motu Cordis}]{A drawing taken from Harvey's \textit{Exercitatio Anatomica de Motu Cordis et Sanguinis in Animalibus} depicting an experiment on valves in the venous circulatory system.}
\label{fig:harvey}
\end{figure*}

Historically, the disease of hypertension was described as a "fullness disease", resulting from an excess of blood in a sufferer's vessels. Early Persian medical texts described the symptoms as including; headaches, general redness, being warm to the touch, distension of the skin, vascular rupture, and even hemorrhagic stroke \cite{Heydari2014}. A natural treatment option therefore took the form of blood-letting or the application of leeches to balance the perceived overabundance of blood present \cite{Esunge1991}. 

It was not until the late $19^{th}$/ early $20^{th}$ century that hypertension gained true clinical importance, catalysed by the invention of the cuff-based sphygmomanometer by Scipione Riva-Rocci in 1896 \cite{Riva-Rocci1896}. This novel device was expanded upon shortly after by Russian surgeon Nikolai Korotkov. Korotkov improved upon Riva-Rocci's approach through the use of a stethoscope to listen to a series of audible sounds when the arteries were auscultated alongside the deflating arm cuff of the sphygmomanometer. Korotkov noted these sounds as being indicative of changes in blood pressure when the heart muscles were contracted (Systole) and relaxed (Diastole), allowing for physicians to discern between the two in a clinical setting \cite{Shevchenko1996}. Armed with the knowledge of these new diagnostic "Korotkov sounds", as they become known, physicians could begin to pick apart the complex relationship between blood pressure and health; and in 1913, Theodore Janeway became the first person to describe how individuals with an increased blood pressure were statistically more likely to die earlier than those with a lower blood pressure \cite{10.1001/archinte.1913.00070060147012}. \\

Today, hypertension is clinically classed with the single diagnostic criteria of chronically elevated \acrfull{bp}. However, in reality hypertension presents a multifactorial disease state with a significant impact on a range of physiological processes. \acrshort{bp} is a normally distributed quantitative trait across the general population, with normal arterial \acrshort{bp} (normotensive) falling within the range of 100 – 140mmHg at systole and 60 – 90mmHg at diastole. The disease of hypertension becomes diagnosed when \acrlong{bp} is measured above the upper end of the normal distribution over multiple time points. \footnote[2]{These multiple measurements are essential in ensuring elevated \acrshort{bp} is a chronic symptom and not one resultant of a patient's anxiety at a clinical visit; a phenomenon known as “White coat syndrome”.} 

Current European guidelines define hypertension when measurements are consistently over 140/90mmHg systolic and diastolic respectively, whereas the latest American guidelines have moved this threshold down to over 130/80mmHg \cite{Williams2018,Whelton2018}. This new definition of hypertension within the American guidelines is based largely on the results derived from large meta-analyses such as SPRINT (Systolic Blood Pressure Intervention Trial) \cite{doi:10.1056/NEJMoa1511939}. In contrast to this, the european guidelines are based on population attributable risk statistics. However, both guidelines suggest a therapeutic end goal of a \acrshort{bp} under 130/80mmHg. In 2003, an additional designation of "Pre-hypertension" was termed for individuals with a \acrfull{sbp} between 120-139mmHg and a \acrfull{dbp} of between 80-89mmHg in order to catch those at risk of further elevations of \acrshort{bp} \cite{Chobanian2003}. This additional designation allows for prophylactic measures to be put in place in order to slow or stop the rise in BP before it leads to a higher risk of cardiovascular disease; either through clinical, or lifestyle intervention. In reality, these classifications provide an arbitrary series of thresholds against a normally distributed range of \acrshort{bp} within the population. As such, they only serve as a guide that may well be subject to change, as their primary purpose is to identify those at risk of the disease states associated with hypertension. Thereby enabling healthcare professionals to intervene accordingly. 

As BP begins to rise, so too do the risk factors associated. Elevated blood pressure significantly increases one's susceptibility towards; stroke, ischemic heart disease, and peripheral vascular disease \cite{Lewington2002,Dickinson2003,Singer2008}. Additional associated diseases include; aortic aneurysms, diffuse aetherosclerosis, atrial fibrillation, pulmonary embolism, and chronic kidney disease \cite{Lau2017,Tracy2002}. These diseases are not limited to the heart and kidneys, as hypertension is also a serious risk factor for retinopathy, and even generalised cognitive impairment and dementia \cite{Tzourio2007,Jenkins2015}. It is therefore not surprising that hypertension is classed as the most important preventable risk factor for premature death in the world \cite{who2009}. 

\subsection{The Hypertensive Pandemic - A Global Burden}

Globally, hypertension is responsible for approximately 9.4 million deaths every year and causes around 45\% of heart disease related deaths and 51\% of deaths due to stroke \cite{WHO}. In 2008, around 40\% of adults over 25 years old were diagnosed with hypertension, a significant rise from 600 million in 1980 to approximately 1 billion. This increasing prevalence has been attributed to an ever-aging population in addition to increased behavioural risk factors, such as poor diet or sedentary lifestyle \cite{WHO2013}. The \acrfull{who} estimates that this number will continue rising to 1.5 billion by 2020 \cite{Kearney2005}. As of 2001, sub-optimal blood pressure was estimated to cost healthcare providers \$370 billion per year globally; a number that represents about 10\% of the world's overall healthcare expenditure \cite{Gaziano2009}. 

\subsection{A Genetic Basis for Hypertension}

For a long time it was unknown whether the genetic basis for BP was a quantitative trait, or a dichotomous disease state. These perspectives gave rise to the famous Platt-Pickering debate during the 1950's, a discussion on the unimodal or bimodal distributions of \acrshort{bp} in the general population. Platt regarded hypertension as a distinct condition, showing a bimodal distribution within the population \cite{PLATT1947}. Whereas Pickering saw \acrshort{bp} as a continuous distribution, with some reaching the clinically set threshold levels of hypertension (Figure ~\ref{fig:plattvspickering} \cite{Pickering1959}). Platt determined that hypertension was a manifestation of simple Mendelian disease inheritance, grounded in a single genetic mutation that gives rise to the hypertensive phenotype. As the debate continued well into the 1960's, Pickering's quantitative model gained little support. It was not until increasing epidemiological evidence suggested the link between elevated \acrshort{bp} and cardiovascular disease that this changed. The ability of a range of pharmaceutically distinct anti-hypertensive drugs to reduce BP, and therefore cardiovascular disease, all strengthened Pickering's normally distributed model. Furthermore, \acrfull{gwas} cemented this within the scientific community, as they showed the importance of considering the polygenic association of genes with hypertension \cite{Ehret2011,Kato2011,Levy2009} \footnote[2]{It should be noted, that Platt's model cannot be entirely disregarded as there exist rare familial forms of hypertension with a Mendelian root, and therefore inherited as single-gene disorders \cite{Lifton2001}.}.

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{Introduction/PlattvsPickering.png}
\caption[The Platt-Pickering debate over the qualitative or quantitative nature of hypertension]{The Platt-Pickering debate over the qualitative or quantitative nature of hypertension. Platt considered hypertension to be a distinct condition, and therefore one resulting of Mendelian genetics. Whereas, Pickering saw \acrfull{bp} as a continuum amongst the general population, with hypertension arising in those at the extreme end of the distribution. He therefore concluded there to be a polygenic aetiology to the disease. Image adapted from Padmanabhan \textit{et al.} 2012 \cite{PADMANABHAN2012397}}
\label{fig:plattvspickering}
\end{figure*}

Today hypertension diagnoses are split into those where a discernible cause is known (Inessential or Secondary Hypertension), and those whereby the specific cause of hypertension can not be identified (Essential or Primary Hypertension). Secondary hypertension only makes up a small proportion of hypertensive cases, at between 5-10\% \cite{Chobanian2003,Omura2004,Vongpatanasin2014}. It can be caused by a wide variety of conditions, including; renal disease, endocrine disorders, and side effects from pharmaceuticals. Due to the tangible nature of the disease, therapeutic measures can be put in place to target the cause of the disorder and effectively manage \acrshort{bp} back to physiological levels. 

The vast majority of hypertensive cases however (90-95\%), have no known cause to their chronically elevated \acrshort{bp}. This grouping of individuals are clinically known as those with Primary Hypertension. Primary hypertension accounts for the highest proportion of cardiovascular disease and is considered to be polygenic in nature \cite{Cowley2006}.

BP regulation is a complex physiological process, governed by an intricate network of interacting pathways that culminate in changes to cardiac output or peripheral resistance. For these reasons, dissecting the genetic underpinnings of the hypertensive phenotype presents a unique set of challenges. Previous studies have demonstrated the genetic contribution towards \acrshort{bp} management. Based on familial studies, specifically those focusing on monozygotic and dizygotic twins, it is estimated to have a heritability rate of between 31-68\% \cite{Hottenga2005,Kupper2005}. It is for these cumulative reasons that isolating the precise genetic cause towards hypertension has proven so difficult. 



\subsection{The Brain's Role in Hypertension} \label{brainhypertension}

The mechanisms by which humans regulate mean arterial BP are not completely understood. While they can be broadly grouped into distinct regulatory systems, in reality they are interrelated. One key regulatory system is that of the \acrfull{raas} which centres on production of angiotensin II, a potent endogenous vasoconstrictor \cite{Arumugam2016}. Over the last few decades significant progress has been made to develop a series of treatments for hypertension, many focusing on modulation of the \acrshort{raas}. However, around 40\% of hypertensive patients remain unresponsive to pharmaceuticals of these classes \cite{Primatesta2001}. This coupled with the established associations between an elevated \acrfull{sns} and hypertension implicate the neurogenic basis of the disease. One key system at play is that of the baroreceptor reflex. Briefly, the baroreceptor reflex involves afferent projections from arterial baroreceptors to \acrfull{cns} pathways (Figure \ref{fig:neurocontrol}). These in turn, effect their inhibitory or excitatory processes on sympathetic outflow, ultimately culminating in alterations to heart rate and cardiac output, in addition to influencing the long term activity of the \acrshort{raas} \cite{Mann2003}. The baroreceptor reflex is primarily tasked with short term BP fluctuations, responding to tissue’s metabolic demand throughout a day. However, it is thought that chronic hypertension arises when alterations to the baseline of this reflex are modified, effectively resetting the homeostatic set-point of BP to an elevated mean arterial pressure.

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{Introduction/NeuroControl.pdf}
\caption[Schematic overview of central nervous system regulation of \acrshort{bp}]{Schematic overview of the \acrfull{cns} control of sympathetic outflow and ultimate regulation of \acrshort{bp}. \acrfull{cns} outflow is the result of integrated actions from a collection of CNS regions, including several centres of the cortex, lower centers of the hypothalamus, circumventricular organs (Area Postrema, AP; Periventricular Anteroventral Third Ventricle, AV3V), and basal ganglia (especially the Locus ceruleus). The critical integrator site for these regions is the nucleus tractussolitaries (NTS), situated within the medulla oblongata. This site receives inhibitory signals from baroreflexes (pressure and volume signals) and stimulatory signals from muscular and renal chemoreceptors (metabolic signals). The ultimate determiner of \acrfull{sns} outflow is the rostal ventrolateral medulla (RVLM), which receives tonic inhibition from the adjacent NTS. The circumventricular organs, such as the AP and AV3V, are devoid of the blood brain barrier and therefore are of particular interest for their role in sensing hormonal and metabolic signals from the blood. The complex integrated signalling taking place centrally ultimately results in the RVLM sending signals via the spinal cord and sympathetic ganglia to regulate stroke volume (SV), heart rate (HR), and systemic vascular resistance (SVR). Image adapted from Chopra \textit{et al.}, 2011 \cite{Chopra2011}}
\label{fig:neurocontrol}
\end{figure*}

For these reasons, the search for blood-born biomarkers has been supplemented by looking toward an additional \acrshort{cns} region of the hypothalamus involved in this pathway; the \acrfull{pvn}. The \acrshort{pvn} is located lateral to the third ventricle and can be further differentiated into eight distinct groups of nuclei in rats consisting of either large magnocellular or smaller parvocellular neurons \cite{Hindmarch2010}. These parvocellular projections directly influence sympathetic tone via the rostalventrolateral medulla (RVLM) and the intermediolateral cell column \cite{Badoer2001}. The magnocellular neurons are tasked with the production of oxytocin and vasopressin, the latter being key for osmoregulation via secretion from the pituitary. Moreover, the PVN is also involved in several other physiological functions, including; response to biological stressors and energy homeostasis \cite{Badoer2001,Scott1998,Tung2008}. All of which underpin these regions as key regions of interest when studying the neurogenic basis of hypertension. 

\section{Animal Models in Hypertension}

When trying to elucidate the aetiology, and any biomarkers that should precede the onset of hypertension, animal models can prove invaluable. There exist several animal models of hypertension, each with their own aetiology and resulting disease characteristics. Therefore, a great level of care is required when selecting one to study that best reflects essential hypertension in humans. 

\subsection{Surgical Models of Hypertension}

One commonly used model in hypertension research is that of the \acrfull{2k1c}. The model requires surgically constricting one renal artery in the rat, leading to a chronically reduced renal perfusion with the contralateral kidney remaining unmodified. The model is characterised by an early and rapid rise in plasma renin in response to the reduced renal artery pressure followed by a resulting increase in circulating Angiotensin II (Ang II) \cite{Wiesel1997}. The \acrshort{2k1c}/\acrshort{1k1c} models are considered robust models of hypertension. However both require invasive surgery to induce a hypertensive state putting a high degree of stress on the animal, and arguably not mimicking the aetiology of human hypertension.

\subsection{Genetic Models of Hypertension}

One of the most widely used rat models of hypertension is that of the \acrfull{shr}. Originally developed in the 1960’s by Okakamoto and Aoki, the strain was obtained by inbreeding Wistar rats that exhibited the highest \acrshort{bp}. After only a few generations the result was a strain of rat that spontaneously begins to develop increased \acrshort{bp} at around 5-6 weeks of age until plateauing at a systolic \acrshort{bp} of 180-200mmHg. This is in contrast to the accepted normal blood pressure of between 80-100mmHg \cite{Okamoto1963}. The importance of this strain resides in its onset of hypertension without the need for pharmacological, surgical or dietary intervention (contrary to models such as the Two-Kidney One-Clip or Dahl Salt-Sensitive rats). 

For this reason, the \acrshort{shr} presents a model very distinct from the others; one that mimics the polygenic aspect of human hypertension. Furthermore, as with any accurate model of a disease state, fully hypertensive adult \acrshort{shr}s develop many of the hallmarks of end-organ damage associated with hypertension in humans. These include; cardiac failure (approx. 60\% of SHRs develop heart failure after 18 months of age), cardiac hypertrophy (approx. 30\% increase in size), and proteinuria from renal dysfunction \cite{Pinto1998,Conrad1991,Pfeffer1979}. The \acrshort{shr} also exhibits impaired endothelium dependent relaxation, a feature of human hypertension that is commonly exhibited and portends the cardiovascular risk that often occurs when left untreated \cite{Luscher1990}. Further support for using the \acrshort{shr} as a proxy to human hypertension is given by its responsiveness of many of the pharmaceutical classes that are used on humans. For example; \acrfull{raas} inhibitors, calcium antagonists, and vasodilators \cite{Wada1996,Takenaka1985,Limas1984}. However, \acrshort{shr}s are less responsive to diuretics and endothelin antagonists, and the data on the effect of beta-blockers has proven unequivocal \cite{Wada1996, Limas1984, Karam1996}. The spontaneous nature of \acrshort{shr} hypertension coupled with its polygenic aetiology make it an ideal candidate to study the genetic basis of human hypertension. At 4 weeks old (juvenile), \acrshort{shr}s present a prehypertensive age, and therefore an age of interest when trying to assess key differences in their transcriptome that could predict their inevitable rise in \acrshort{bp}. 

\subsection{Reconciliation of the Genome-Environment Interface}

Unlike monogenic diseases, complex polygenic disease states are much more likely to be a function of the environment. Hypertension is no different in this respect, as multiple modifiable risk factors towards the disease have been identified. It is therefore important to consider the interactions between a genetically predisposed individual, and the lifestyle factors that may contribute to a hypertensive phenotype. One such modifiable risk factor in the development of hypertension is salt intake \cite{Stamler1997}. While the specific guidelines towards recommended salt intake have varied in light of contradicting studies, there is a clear relationship between a high salt diet and elevated blood pressure \cite{Ha2014}.  

With this is mind the Dahl salt-sensitive rats represent another commonly used model of hypertension. In the 1950's, Meneely \textit{et al.} began to study the effects of a high salt diet on the \acrshort{bp} of rats. The team noted a linear relationship between the percentage of NaCl (between 0 and 10\%) within the diet and the average \acrshort{bp} measured within a cohort of \acrfull{sd}. They further noted the marked degree of variation between individuals in their \acrshort{bp} response to salt \cite{MENEELY1958}. Then in the 1960's, Dr. Lewis Dahl built upon the observations of Meneely \textit{et al.} by studying the effects of chronic excess of salt in rat models \cite{DAHL1961}. Dahl took advantage of the observation that rats had a differential response to salt. Being mindful of the data suggesting genetic influences on blood pressure in humans, he set about selectively breeding rats either susceptible (Dahl-Salt Sensitive) or resistant (Dahl-Salt Resistant) towards the hypertensive effects of a high salt diet (8\% NaCl). Dahl \textit{et al.} managed to clearly separate these lines after only three generations of breeding. The blood pressure of Dahl-Salt Resistant animals were effectively similar between those fed a control diet or a high salt diet. However the Dahl-Salt Sensitive animals develop severe and fatal hypertension under a high salt diet \cite{DAHL1962,DAHL1962a}. 

Although the organ damage endpoints are comparable to many other models of hypertension, with cardiac hypertrophy (approx. 32\%) and eventual cardiac failure (4-5 months of age) robustly observed, the model requires a dietary intervention before these become apparent \cite{Pinto1998}. The Dahl Salt-Sensitive and resistant animals constitute a hugely important model of studying blood pressure regulation, as they present an opportunity for reconciling the genome-environment interface in the aetiology of hypertension.

\section{The Hunt for Biomarkers}

Often referred to as a “silent killer”, hypertension is asymptomatic in its early stages. This results in many patients remaining undiagnosed (1 in 3) until the impact becomes severe and possibly fatal, highlighting the need for improved diagnostics \cite{PHE2013}. Should elevated \acrshort{bp} remain unmanaged, it becomes a serious risk factor for the diseases outlined prior. In order to effectively manage hypertension, the need to intervene before associated risk factors develop is paramount. For that reason, it is important to identify a series of biological markers, or biomarkers, that precede the onset of the disease. These characteristic changes can be at any level of biology, provided they serve as an objective metric for the progression of the disease accurately and reproducibly. Another crucial consideration of a clinically relevant biomarker is its accessibility. Blood offers an accessible tissue that can be obtained through relatively non-invasive means. Here, the search for prognostic biomarkers is centred on the biological level of the blood transcriptome. This is due to the ability of RNA to reflect the biological state of the individual in a highly dynamic manner; being able to integrate both the genetic and epigenetic mechanisms at play in the aetiology of the disease. Indeed many studies are looking towards transcript expression in offering a robust biomarker for a disease state due to their early and more accurate predictive properties \cite{Sunde2010,Bazzell2018,10.1093/sleep/zsy186}. 

\section{Transcriptomics}

The transcriptome is the total sum of transcripts and their respective quantities within a cell. These can include all of the mRNAs, small RNAs and non-coding RNAs a cell is actively expressing. 
Focusing the search on the transcriptome allows for a dynamic metric of gene expression, allowing a reconciliation between an organism’s genetic predisposition towards a disease, and how that is being affected due to environmental stimuli. 

\subsection{Evolution of Nucleotide Sequencing}

In 2003, the complete mapping of the human genome gave rise to a new era in genetic analysis. The project sought to sequence a representative version of each human chromosome (approximately 3 billion bases in total). 

The base resolution of libraries was resolved via a process based on Sanger Sequencing. From its conception in 1977, Frederick Sanger's eponymous sequencing technique was a ground breaking development in the ability to unravel the genetic code of life \cite{Sanger1977}. The central development in the technique was the use of specific chain-terminating inhibitors of deoxynucleotide Triphosphates (dNTPs) to prevent the elongation of oligonucleotides. DNA templates that were to be sequenced were combined with DNA polymerase, free deoxynucleotide bases (dATP, dCTP, dGTP, and dTTP), and separated out into 4 separate incubations, each with sparse mixture of one radioisotope labeled dideoxynucleotide base (ddATP, ddCTP, ddGTP, and ddTTP). Following a heating step to denature the template DNA, a cooling step allows the DNA primer to anneal. DNA polymerase could then synthesise a complementary DNA strand one base at a time. These bases are randomly incorporated until a dideoxynucleotide base was utilised, terminating the DNA elongation. The resulting mixture was one of newly synthesised DNA strands, each differing in length by a single nucleotide and all labeled at their 3' end with a radioisotope. In order to read the sequences researchers could separate the new DNA strands based on size through electrophoresis as each strand differed by a single DNA molecule. By running all strands through an electrophoresis gel, the resulting pattern of labeling on radiation sensitive film could be read one base at a time, taking note of which original incubation it belonged to to therefore to resolve the base for that read \cite{Pareek2011}. 

The whole process was painstakingly slow, and to this day remains the world’s largest collaborative biological project taking a total of 13 years to reach completion. This huge undertaking relied on a massively parallel and admirably driven workforce consisting of 20 institutions and span across 6 nations \cite{NHGRI}. As a result, the whole process cost an estimated \$2.7 billion to yield the first draft sequence \cite{Pareek2011}. 


\subsection{Next Generation Sequencing}

Today an individual’s genome can be resolved in as little as a day. This huge reduction in time is due to a coevolution of massively parallel sequencing technology, and an exponential development in the computational power required to process it. Furthermore, these rapid advances have resulted in a marked reduction in associated costs.  The downward trend in the price to sequence the human genome has far surpassed Moore’s law, a metric used to assess technological improvements as computational power advances. These advancements have been termed as "Next-Generation Sequencing (NGS)". They expand upon the procedural concepts outlined by Sanger Sequencing and translate them to a massively-parallel approach. While many variations of NGS technology exist, one of the main approaches employs the \acrfull{sbs} method. 

The procedure begins with the random fragmentation of genomic DNA, or cDNA, to produce a pool (Library) of uniformly sized fragments. A series of adapters are ligated to each end of the fragments, as well as unique stretches of oligonucleotides for later identification of the original sample from which the fragments were derived ("Barcoding"). Each library is mixed and washed over a flow cell where a lawn of bound oligonucleotides bind the complementary adapters that flank the sample fragment.  
At this point cluster generation is required to yield clusters of similar fragments such that their cumulative signal is sufficient to be detected by the sequencer. This can take place within the sequencer machine (as an onboard cluster module), or on an external machine (such as the cBot, Illumina, Inc. - San Diego, CA, USA). This is achieved via a process known as "Bridge Amplification", and begins with the opposite end of a ligated fragment bending over and "bridging" to another complementary oligo bound to the flow cell lawn. Through repeated denaturation and extension cycles (similar to a \acrfull{pcr}), fragments are locally amplified from single molecules into millions of unique, clonal clusters along the flow cell. At this point the \acrshort{sbs} process can take place.

\acrshort{sbs} technology classically uses four fluorescently labeled nucleotides to sequence millions of clusters bound to the flow cell in parallel \footnote[2]{Illumina's NextSeq Series introduced the concept of 2-channel \acrshort{sbs} technology, with only two flourophores required. Here only two images are needed to resolve each base, contrary to the classical 4 images employed before. This is achieved by using a red and green wavelength filter to identify colours as red (C), green (T), a combination (yellow, A) or no colour, given (G). This concept has radically sped up sequencing runs but has raised concerns about accuracy.}. During each of the sequencing cycles, a single complementary \acrfull{dntp} binds to the oligonucleotide chain, and reversibly terminates strand elongation. After \acrshort{dntp} incorporation the bound fluorophore is excited and detected through laser imaging to resolve the specific base that was incorporated within that cluster. The reversible terminator \acrshort{dntp} can then be enzymatically cleaved ready for the next cycle of incorporation. 

By the very nature of NGS, huge amounts of data are produced from any given experiment. Depending on the organism being studied, genomes can vary in size from the smallest non-viral genome at 160Kb, to the largest known at 670GB. The complexity of the transcriptome represents its own challenge with regions of the genome being expressed more than once; giving rise to an additional dimension of data to interpret. 

The ability to deal with this scale of “Big Data” in an acceptable length of time is incredibly computationally demanding. To maintain a feasible time frame for any given project, focus needs to be given to optimising mathematical approaches to best optimise available computational power. 

\section{Bioinformatics Challenges}

The chaining of processes required to take the raw .Fastq sequencing files to a final biologically interpretable output is referred to as a “Pipeline” (Figure \ref{fig:pipeline}). Currently, there are no "gold standards" in optimal pipelines for differential expression. Different laboratories favour different pipelines depending on the organism being studied, their research goals, or simply their familiarity with the software.


\begin{figure*}[!htbp]
\centering
\includegraphics[width=1\textwidth]{Introduction/pipeline.pdf}
\caption[Example Pipeline for RNAseq Experiment]{Example Pipeline for RNAseq Experiment. Here, each step is shown from raw demultiplexed reads to final biologically interpretable output files. Some examples of software programs are shown for each step. Also displayed are the common file types associated with each stage in the pipeline.}
\label{fig:pipeline}
\end{figure*}

\subsection{Quality Control}

Bioinformatic \acrfull{qc} of outputted NGS reads is equally as essential as the QC steps taken at the library preparation stage. These QC checks should be employed pertinently at several stages along the bioinformatic analysis. The very first step in any RNAseq pipeline is the QC analysis of the raw reads to assess; sequence quality, \acrfull{gc} content, overrepresentation of particular subsequences (k-mers, where \textit{k} equals the length of the sequence in bases), duplicated reads and the presence of the adapters used during the library preparation stage. Each of these metrics can be indicative of a potential error that may have confounding effects on the final results. To this end, FastQC provides a comprehensive software package for Illumina-generated reads to produce graphical plots of the above analyses \cite{Andrews2010}. 

While the technology used for the Human Genome Project has since become outdated for experiments requiring a high-throughput approach, its legacy has given rise to a series of novel assessments that remain in use today. The most common metric for base calling accuracy is the Phred Quality score. The Phred score was originally developed as an algorithmic approach towards resolving Sanger sequencing reads, taking into account the peak resolution and shape of the detected fluorophore signal. While these attributes no longer exist in next-generation sequencing technologies, the process of generating Phred scores has remained largely the same. Using different sets of parameters, relevant to the specific sequencing chemistry being used, accuracies are correlated against a large lookup data set of known accuracies (Figure \ref{eq:phred}). \\

\begin{figure}[!htbp]
\LARGE
\[Q=\log_{10}p\]
\normalsize
\begin{align*}
\text{Where;} ~Q\ &= \text{Phred Quality Score} \\
~p\ &= \text{Probability of an incorrect base}
\end{align*}
\caption[Equation for calculating Phred Score]{Equation for calculating Phred Score. Here, probabilities (\textit{p}) are retrieved from pre-calculated lookup tables of known accuracies.}
\label{eq:phred}
\end{figure}

The resulting Q-Score is one that represents the probability of an incorrect base call after any given number of bases resolved (Table \ref{tab:phred}). Lower Q-scores can increase levels of false positive variant calling, resulting in inaccurate conclusions or higher costs due to additional validation experiments. Today, the "gold standard" of sequencing experiments is a Q-score of 30 (Q30), which translates to a probability of 1 potential miscall in 1,000 bases.  

Phred scores have proven to be incredibly consistent throughout a series of different sequencing technologies, leading to it becoming the industry standard for all commercial sequencing technologies \cite{Richterich1998}. \\


%\begin{split}
%\[Q = -\log_{10}p}\]
%\end{split}
%\begin{align}
%\normalsize
%\text{Where;} ~Q &= \text{Phred Quality Score} \\ 
%~p &= \text{Probability of an incorrect base} \\
%\caption[Equation for calculating the Phred Score]{Equation for calculating the Phred Score}
%\end{align}


%\equationset{\textnormal{Equation for calculating Phred Score}}

%\begingroup\vspace*{-\baselineskip}
%\captionof{figure}{Equation for calculating Phred Score}
%\vspace*{\baselineskip}\endgroup

%\begin{align*}
%\begin{split}
%\huge
%\centering
%Q = -\log_{10}P \\
%\normalsize
%\begin{align*}
%\text{Where;} ~Q &= \text{Phred Quality Score} \\ 
%~p &= \text{Probability of an incorrect base} \\
%\end{align*}
%\end{split}
%\caption[Equation for calculating the Phred Score]{Equation for calculating the Phred Score}
%\label{eq:phred}
%\end{align*}

\begin{table}[!htbp]
\centering
\begin{tabular}{lrr}
\textbf{Phred Quality Score} & \begin{tabular}[r]{@{}r@{}}\textbf{Probability of} \\ \textbf{incorrect base call}\end{tabular} & \textbf{Accuracy of base call} \\
\hline
10                  & 1 in 10                                                                          & 90\%                  \\
20                  & 1 in 100                                                                         & 99\%                  \\
30                  & 1 in 1,000                                                                       & 99.9\%                \\
40                  & 1 in 10,000                                                                      & 99.99\%              
\end{tabular}
\caption[Table of Phred Scores against Base Miscalls]{Table of example Phred scores and their accompanying base calling accuracies.}
\label{tab:phred}
\end{table} 


\subsection{Trimming} \label{Trimming}

It is essential to remove the adapters added to each fragment during the library preparation stage. A trimming software package (e.g. Trimmomatic or BBDuk) can be used to trim low quality bases, trim adapter sequences, or simply discard poor quality reads. All of which will improve mappability to a reference genome. However, it must be noted that the more aggressive the trimming approach, the greater loss of potentially important information contained within the reads. For this reason, a balance needs to be struck in order to get the most from the sequencing experiment. 

Furthermore due to a phenomenon known as “Phasing”, read quality classically falls towards the 3’ end of reads \cite{EcSeq2017}. This occurs when the nucleotide blocker, employed during each sequencing by synthesis chemistry cycle, fails to be removed after signal detection. During the next cycle of chemistry, a nucleotide is prevented from binding the template DNA fragment, and the old nucleotide is once again detected. The fluorescent signal of the old nucleotide most probably differs from the signal produced around it from homologous template strands and is therefore not a synchronous signal. For the rest of the sequencing procedure the result will be one cycle behind the detection signal of the other strands of that homologous cluster, polluting the signal detected by the sequencer's camera and preventing a high confidence base call for that particular cluster. This phenomenon occurs with a relatively low probability. However over time this can accumulate and begin to pollute the detected light signal more and more. This is especially true with increasing read lengths. Sequence quality plots can quickly identify if this quality becomes too low across the read allowing the bioinformatician to make the decision whether to remove these low-quality bases (Figure ~\ref{fig:phasing_illumina}).

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.7\textwidth]{Introduction/phasing_illumina.png}
\includegraphics[width=0.7\textwidth]{Introduction/phasing.png}
\caption[Phasing Phenomenon observed within Illumina's Sequencing by Synthesis technology]{Phasing Phenomenon observed within Illumina's Sequencing by Synthesis technology (Illumina, Inc. - San Diego, CA, USA). Phasing occurs when a blocker nucleotide is not removed during one of the chemistry cycles of the experiment (Above, Image taken from ECSeq Bioinformatics \cite{EcSeq2017}). Downstream signal is therefore asynchronous within the homologous cluster, and leads to a decreasing confidence in accurate base calling, as represented by a reduction in Phred score (Below, BoxWhisker type plot generated by FASTQC (v0.11.7) \cite{Andrews2010}).}
\label{fig:phasing_illumina}
\end{figure*}

\subsection{Read Alignment}
Once reads have been filtered or trimmed for optimum quality, they can be aligned to either a reference genome or transcriptome. Either present a series of pro’s and con’s to be assessed on an experiment by experiment basis. An important metric for read alignment success is simply the percentage of reads successfully mapped to the reference genome/transcriptome. Depending on how stringent the aligner algorithm, this  can provide an overall indicator of the sequencing accuracy, and whether or not any contaminating DNA is present. As an example, an expected rate of between 70 and 90\% of RNAseq reads should map to the human genome with some of those reads mapping to several identical regions within the reference (known as "Multi-Mapping Reads") \cite{Dobin2012}. It would be expected that a lower level of total reads map when mapping to the transcriptome, as unannoted regions will be lost. As a consequence more reads become defined as multi-mapping due to the number of reads falling onto shared exons from the same genes. For these reasons, priority has been given to mapping reads to the reference genome rather than the transcriptome. 

Additional parameters that need to be considered are the uniformity of reads across exons, as reads that primarily accumulate at the 3' end of poly(A) selected samples may be indicative of low quality starting RNA. 

\subsection{Transcript Quantification and Normalisation}

Most analyses of RNAseq data require an estimation of the number of genes expressed, usually for later \acrfull{dge} analysis. The most basic approach to quantifying expression levels is to simply aggregate the number of raw counts of mapped reads, through programs such as FeatureCounts, or HTSeq-count \cite{Liao2014,Anders2015}. This process makes use of a \acrfull{gtf} file, containing genomic coordinates for genes and their exons, to quantify the number of reads that fall within these boundaries. However, these "raw" read counts are not sufficient for comparing the expression of transcripts between samples, as they are greatly affected by a series of confounding factors inherent in both library preparation and RNAsequencing. These include factors such as; sequencing depth, transcript length, and inherent biases in RNAseq technlogy. Therefore, read normalisation is essential before read counts can be used downstream with any confidence. 

The process of read normalisation is one that has presented the greatest variety in approaches. In order to convert raw read counts into a measure of relative abundance that can be biologically interpreted, normalisation is an essential step. It is this crucial nature that has led normalisation to gain so much research interest. As so many factors can lead to a potentially confounding effect on the final output, there is no single normalisation strategy that appears to adequately deal with all of these factors. 

One of the early methods to address these confounders was to perform a within-sample normalisation, by dividing the raw count by the number of kilobases in its exon model, and again by the total number of reads generated the sequencing experiment. The resulting measure was called the \acrfull{rpkm} \cite{Mortazavi2008}. The \acrshort{rpkm}, and its derivatives such as the analogous \acrfull{fpkm}, were for a long time the most frequently reported gene expression values for RNAseq \footnote[2]{With single-ended reads, both \acrshort{rpkm} and \acrshort{fpkm} are equivalent. As paired-end experiments generate two reads that correspond to a single fragment, the latter of the two was created as a way of preventing doubly counted reads per transcript.}. The need to correct for gene length is only necessary for correctly assessing gene expression levels within the sample as compared with other genes. However, it it not necessary when comparing any changes to transcript expression within the same gene across samples as they will be subject to the same bias of longer genes accumulating more reads. For this reason, the additional \acrfull{tpm} normalisation method is often also used \cite{Conesa2016}. 

\subsection{Differential Gene Expression}

The key to \acrfull{dge} analysis is the comparison of gene expression across samples. A significant problem with the above methods is that they normalise away the sequencing depth of the experiments which can differ between the samples. The above methods rely on normalising total counts under the assumption of a homogeneous transcript distribution. They can therefore perform poorly should any sample contain a heterogeneous distribution of transcripts, as highly expressed transcripts can skew the distribution of counts \cite{Bullard2010,Hansen2010,Conesa2016}. All of the newer DGE analysis suites utilised here offer their own forms of normalisation, and all ignore features that are either highly expressed or highly variable when calculating normalisation factors. The following DGE analysis software suites aim to successfully deal with differing library composition.

\subsubsection{\acrfull{edger}}
One of the more popular DGE analysis suites is EdgeR, from Robinson \textit{et al.} \cite{Robinson2010}. EdgeR utilises the \acrfull{tmm} approach towards normalisation, whereby a single sample is selected as a reference. It selects this sample by creating a library size scale for each gene for each sample, and calculating the average $75^{th}$ quantile across all samples. The sample with the $75^{th}$ quantile closest to the average value becomes the reference sample for downstream fold change calculations. Geometric means of fold changes (biased genes) and absolute expression levels (highly or lowly expressed) are calculated from this raw count before being trimmed twice by thresholds defined by these values. In other words, the most "extreme" genes are removed before calculating the weighted scaling factors for each individual gene. 

\subsubsection{\acrfull{deseq} \& DESeq2}
Both DESeq and DESeq2 take the approach of finding a ratio of each raw read count to the geometric mean of all read counts for that specific gene, across all samples. Here the denominator becomes the pseudo-reference sample by which all samples are scaled by. As in EdgeR, by using the geometric mean extreme outliers have less impact in the resulting scaling factors \cite{Anders2010,Love2014}. \\

A series of independent studies have shown the choice of DGE analysis approach to have the potential to markedly affect the outcome of the data, and that no specific package used is likely to yield favourable results for all experiments \cite{Soneson2013,Rapaport2013, Seyednasrollah2015}. For this reason, a great deal of care is employed for selecting an apropriate DGE analysis approach for this body of work.

\section{Statement of Intent and Aims}

It is therefore the aim of this body of work to use next-generation sequencing to profile the transcriptome of a genetic model of human hypertension. By identifying biomarkers of the disease, it may be possible to develop a prognostic test for identifying those at risk and to further treat the individual prior to the onset of the disease, mitigating the organ damage that may otherwise have occured. Using this undirected approach may allow for the detection of novel transcript biomarkers that have previously not been implicated in hypertension. Once a robust biomarker is revealed, efforts will be directed towards further characterising its potential involvement in the disease in the hope of revealing novel mechanistic insights into the aetiology of hypertension. 













